{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading the data\n",
    "df_train_login = pd.read_csv('t_login.csv')\n",
    "df_train_trade = pd.read_csv('t_trade.csv')\n",
    "df_test_login = pd.read_csv('t_login_test.csv')\n",
    "df_test_trade = pd.read_csv('t_trade_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------process data first........"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the login table by id...\n",
    "df_train_login.sort_values(by=['id', 'timestamp', 'result'], inplace=True, ascending=True)\n",
    "df_test_login.sort_values(by=['id', 'timestamp', 'result'], inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_timestamp(x):\n",
    "    # string to datetime with the format\n",
    "    x = datetime.datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\")\n",
    "    # datetime to timestamp\n",
    "    return str(time.mktime(x.timetuple()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort the trade table by id...\n",
    "df_train_trade['time'] = df_train_trade['time'].apply(lambda x: get_timestamp(x[:-2]))\n",
    "df_train_trade.sort_values(by=['id', 'time'], inplace=True, ascending=True)\n",
    "\n",
    "df_test_trade['time'] = df_test_trade['time'].apply(lambda x: get_timestamp(x[:-2]))\n",
    "df_test_trade.sort_values(by=['id', 'time'], inplace=True, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_datetime(x):\n",
    "    if not isinstance(x, float):\n",
    "        x = float(x)\n",
    "    return datetime.datetime.fromtimestamp(x)\n",
    "\n",
    "# 判断target是否为array中的大多数\n",
    "def is_majority(target, array, n):\n",
    "    # 如果候选列表只有一个候选项目，则返回待定值2\n",
    "    if len(array) == 1:\n",
    "        return 2\n",
    "\n",
    "    c = Counter(array).most_common(n)\n",
    "    for item in c:\n",
    "        if item[0] == target:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# 判断target是否在array中至少出现n次\n",
    "def is_exists(target, array, n):\n",
    "    return 1 if array.count(target) >= n else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_0(login, trade):\n",
    "    # features\n",
    "    features = pd.DataFrame()\n",
    "    features['time'] = trade['time']\n",
    "    features['trade_real_month'] = trade['trade_real_month']\n",
    "    features['trade_real_day'] = trade['trade_real_day']\n",
    "    features['trade_real_hour'] = trade['trade_real_hour']\n",
    "    features['trade_real_minute'] = trade['trade_real_minute']\n",
    "    features['trade_real_second'] = trade['trade_real_second']\n",
    "    \n",
    "    print('Shape of Features 0:', features.shape)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature_1(login, trade):\n",
    "    #features\n",
    "    # 初始化特征列表\n",
    "    last_login_time = []\n",
    "    last_login_result = []\n",
    "    last_login_timelong = []\n",
    "    last_login_device = []\n",
    "    last_login_from = []\n",
    "    last_login_ip = []\n",
    "    last_login_city = []\n",
    "    last_login_type = []\n",
    "    last_login_is_scan = []\n",
    "    last_login_is_sec = []\n",
    "    last_trade_time = []\n",
    "    \n",
    "    # 计数特征\n",
    "    count_login_all = []  # 登录的总次数\n",
    "    count_login_previous = []  # 当前时间是第几次登录\n",
    "    count_trade_all = []\n",
    "    count_trade_previous = []\n",
    "    count_login_fail_all = []  # 登录失败总次数\n",
    "    count_login_fail_previous = []\n",
    "    count_login_succ_all = []\n",
    "    count_login_succ_previous = []\n",
    "    avg_login_previous = []  # 上次登录与登录时间平均值的差\n",
    "    max_login_previous = []\n",
    "    min_login_previous = []\n",
    "    count_login_ip_all = []  #     上次登录的ip在所有登录记录中出现的次数，出现次数太多的ip显然是异常的\n",
    "    count_login_device_all = []\n",
    "    count_login_city_all = []\n",
    "    \n",
    "    # 类别特征\n",
    "    is_login_device_usual = []\n",
    "    is_login_from_usual = []\n",
    "    is_login_ip_usual = []\n",
    "    is_login_city_usual = []\n",
    "    is_login_type_usual = []\n",
    "    is_login_device_exists_previous = []  # 上次登录的设备是否在以前出现过\n",
    "    is_login_from_exists_previous = []\n",
    "    is_login_ip_exists_previous = []\n",
    "    is_login_city_exists_previous = []\n",
    "    is_login_type_exists_previous = []\n",
    "    \n",
    "    # 提前计算好一些数量，可以同时利用上测试集和训练集的特征\n",
    "    # Start Pre-Calculating ... \n",
    "    set_ip = Counter(login['ip'].tolist())\n",
    "    set_device = Counter(login['device'].tolist())\n",
    "    set_city = Counter(login['city'].tolist())\n",
    "    \n",
    "    # 开始遍历所有的交易信息\n",
    "    # Start Search All Trade Info ... \n",
    "    for index in range(trade.shape[0]):\n",
    "        if index % 1000 == 0:\n",
    "            print(\"Processing till line: \", index)\n",
    "        each_line = trade.iloc[index]\n",
    "        \n",
    "        # 取出交易记录的id,time等信息\n",
    "        trade_time = each_line['time']\n",
    "        id_ = each_line['id']\n",
    "        \n",
    "        # 查找登录信息中同id的记录\n",
    "        related_rows = login.loc[login['id'] == id]\n",
    "        count_login_all.append(related_rows.shape[0])\n",
    "        count_login_fail_all.append(len([1 for num in     related_rows['result'].tolist() if num != 1]))\n",
    "        count_login_succ_all.append(len([1 for num in     related_rows['result'].tolist() if num == 1]))\n",
    "        \n",
    "        # 存在相同id\n",
    "        if related_rows.shape[0] != 0:\n",
    "            # 查找当前交易发生之前的登录信息\n",
    "            previous_record = related_rows.loc[related_rows['time'] < float    (trade_time)]\n",
    "            count_login_previous.append(previous_record.shape[0])\n",
    "            count_login_fail_previous.append(len([1 for num in     previous_record['result'].tolist() if num != 1]))\n",
    "            count_login_succ_previous.append(len([1 for num in     previous_record['result'].tolist() if num == 1]))\n",
    "            \n",
    "            # 交易前有登录信息\n",
    "            if previous_record.shape[0] != 0:\n",
    "                first_login_record = previous_record.iloc[0]  #     取第一次登录的记录\n",
    "                last_login_record = previous_record.iloc[-1]  #     取最近的那一次登录记录\n",
    "                \n",
    "                last_login_time.append(trade_time -     last_login_record['time'])\n",
    "                last_login_result.append(last_login_record['result'])\n",
    "                last_login_timelong.append(last_login_record['timelong'])\n",
    "                last_login_device.append(last_login_record['device'])\n",
    "                last_login_from.append(last_login_record['log_from'])\n",
    "                last_login_ip.append(last_login_record['ip'])\n",
    "                last_login_city.append(last_login_record['city'])\n",
    "                last_login_type.append(last_login_record['type'])\n",
    "                last_login_is_scan.append(last_login_record['is_scan'])\n",
    "                last_login_is_sec.append(last_login_record['is_sec'])\n",
    "                \n",
    "                # 计数特征\n",
    "                avg_login_previous.append(np.average(    related_rows['timelong']) - last_login_record['timelong'])\n",
    "                max_login_previous.append(np.max(related_rows['timelong'])     - last_login_record['timelong'])\n",
    "                min_login_previous.append(np.min(related_rows['timelong'])     - last_login_record['timelong'])\n",
    "                count_login_ip_all.append(set_ip[last_login_record['ip']])\n",
    "                count_login_device_all.append(set_device[    last_login_record['device']])\n",
    "                count_login_city_all.append(set_city[    last_login_record['city']])\n",
    "                \n",
    "                # 类别特征\n",
    "                is_login_device_usual.append(is_majority(    last_login_record['device'], related_rows['device'], 2))      # 上次登录的设备在所有登录设备中是否为常用设备\n",
    "                is_login_from_usual.append(is_majority(    last_login_record['log_from'], related_rows['log_from'],     2))\n",
    "                is_login_ip_usual.append(is_majority(    last_login_record['ip'], related_rows['ip'], 2))\n",
    "                is_login_city_usual.append(is_majority(    last_login_record['city'], related_rows['city'], 2))\n",
    "                is_login_type_usual.append(is_majority(    last_login_record['type'], related_rows['type'], 2))\n",
    "    \n",
    "                is_login_device_exists_previous.append(\n",
    "                    is_exists(last_login_record['device'],     previous_record['device'].tolist(), 2))  #     上次登录的设备在之前是否登录过\n",
    "                is_login_from_exists_previous.append(\n",
    "                    is_exists(last_login_record['log_from'],     previous_record['log_from'].tolist(), 2))\n",
    "                is_login_ip_exists_previous.append(\n",
    "                    is_exists(last_login_record['ip'],     previous_record['ip'].tolist(), 2))\n",
    "                is_login_city_exists_previous.append(\n",
    "                    is_exists(last_login_record['city'],     previous_record['city'].tolist(), 2))\n",
    "                is_login_type_exists_previous.append(\n",
    "                    is_exists(last_login_record['type'],     previous_record['type'].tolist(), 2))\n",
    "            \n",
    "            # 交易前没有登录信息存在\n",
    "            else:\n",
    "                last_login_time.append(-1)  # 如果没有登录信息，说明用户之前没有    登录，但是后来马上有登录，可信度较大，则赋-1\n",
    "                last_login_result.append(-40)\n",
    "                last_login_timelong.append(-1)\n",
    "                last_login_device.append(0)\n",
    "                last_login_from.append(0)\n",
    "                last_login_ip.append(0)\n",
    "                last_login_city.append(0)\n",
    "                last_login_type.append(0)\n",
    "                last_login_is_scan.append(-1)\n",
    "                last_login_is_sec.append(-1)\n",
    "    \n",
    "                # 计数特征\n",
    "                avg_login_previous.append(0)\n",
    "                max_login_previous.append(0)\n",
    "                min_login_previous.append(0)\n",
    "                count_login_ip_all.append(0)\n",
    "                count_login_device_all.append(0)\n",
    "                count_login_city_all.append(0)\n",
    "    \n",
    "                # 类别特征\n",
    "                is_login_device_usual.append(0)\n",
    "                is_login_from_usual.append(0)\n",
    "                is_login_ip_usual.append(0)\n",
    "                is_login_city_usual.append(0)\n",
    "                is_login_type_usual.append(0)\n",
    "    \n",
    "                is_login_device_exists_previous.append(-1)  #     上次登录的设备在之前是否登录过\n",
    "                is_login_from_exists_previous.append(-1)\n",
    "                is_login_ip_exists_previous.append(-1)\n",
    "                is_login_city_exists_previous.append(-1)\n",
    "                is_login_type_exists_previous.append(-1)\n",
    "        else:\n",
    "            # 这里出现的都是没有出现登录记录的\n",
    "    \n",
    "            # 如果没有同id的信息，说明用户之前没有登录，后来也没有登录\n",
    "            last_login_time.append(-2)\n",
    "            last_login_result.append(-41)\n",
    "            last_login_timelong.append(-2)\n",
    "            last_login_device.append(0)\n",
    "            last_login_from.append(0)\n",
    "            last_login_ip.append(0)\n",
    "            last_login_city.append(-1)\n",
    "            last_login_type.append(-1)\n",
    "            last_login_is_scan.append(-1)\n",
    "            last_login_is_sec.append(-1)\n",
    "    \n",
    "            # 计数特征\n",
    "            count_login_previous.append(0)\n",
    "            avg_login_previous.append(0)\n",
    "            max_login_previous.append(0)\n",
    "            min_login_previous.append(0)\n",
    "            count_login_succ_previous.append(0)\n",
    "            count_login_fail_previous.append(0)\n",
    "            count_login_ip_all.append(0)\n",
    "            count_login_device_all.append(0)\n",
    "            count_login_city_all.append(0)\n",
    "    \n",
    "            # 类别特征\n",
    "            is_login_device_usual.append(0)\n",
    "            is_login_from_usual.append(0)\n",
    "            is_login_ip_usual.append(0)\n",
    "            is_login_city_usual.append(0)\n",
    "            is_login_type_usual.append(0)\n",
    "    \n",
    "            is_login_device_exists_previous.append(-2)\n",
    "            is_login_from_exists_previous.append(-2)\n",
    "            is_login_ip_exists_previous.append(-2)\n",
    "            is_login_city_exists_previous.append(-2)\n",
    "            is_login_type_exists_previous.append(-2)\n",
    "        \n",
    "        # 查找交易信息中同id的记录\n",
    "        related_rows = trade.loc[trade['id'] == id]\n",
    "        count_trade_all.append(related_rows.shape[0])\n",
    "        if related_rows.shape[0] != 0:\n",
    "            # 查找当前交易发生之前的交易信息\n",
    "            previous_record = related_rows.loc[related_rows['time'] < float    (trade_time)]\n",
    "            count_trade_previous.append(previous_record.shape[0])\n",
    "        else:\n",
    "            count_trade_previous.append(0)\n",
    "    \n",
    "        if index > 0:\n",
    "            last_trade_record = trade.iloc[index - 1]\n",
    "            if last_trade_record['id'] == id:\n",
    "                last_trade_time.append(trade_time - float(    last_trade_record['time']))\n",
    "            else:\n",
    "                last_trade_time.append(-1)\n",
    "        else:\n",
    "            last_trade_time.append(-2)\n",
    "    \n",
    "    # 将得到的特征拼接起来\n",
    "    features['last_login_time'] = last_login_time\n",
    "    features['last_login_result'] = last_login_result\n",
    "    features['last_login_timelong'] = last_login_timelong\n",
    "    features['last_login_device'] = last_login_device\n",
    "    features['last_login_from'] = last_login_from\n",
    "    features['last_login_ip'] = last_login_ip\n",
    "    features['last_login_city'] = last_login_city\n",
    "    features['last_login_type'] = last_login_type\n",
    "    features['last_login_is_scan'] = last_login_is_scan\n",
    "    features['last_login_is_sec'] = last_login_is_sec\n",
    "    features['last_trade_time'] = last_trade_time\n",
    "    \n",
    "    # 计数特征\n",
    "    features['count_login_all'] = count_login_all\n",
    "    features['count_login_previous'] = count_login_previous\n",
    "    features['count_trade_all'] = count_trade_all\n",
    "    features['count_trade_previous'] = count_trade_previous\n",
    "    features['avg_login_previous'] = avg_login_previous\n",
    "    features['max_login_previous'] = max_login_previous\n",
    "    features['min_login_previous'] = min_login_previous\n",
    "    features['count_login_fail_all'] = count_login_fail_all\n",
    "    features['count_login_fail_previous'] = count_login_fail_previous\n",
    "    features['count_login_succ_all'] = count_login_succ_all\n",
    "    features['count_login_succ_previous'] = count_login_succ_previous\n",
    "    features['count_login_ip_all'] = count_login_ip_all  #     上次登录的ip在所有登录记录中出现的次数\n",
    "    features['count_login_device_all'] = count_login_device_all\n",
    "    features['count_login_city_all'] = count_login_city_all\n",
    "    \n",
    "    # 类别特征\n",
    "    features['is_login_device_usual'] = is_login_device_usual\n",
    "    features['is_login_from_usual'] = is_login_from_usual\n",
    "    features['is_login_ip_usual'] = is_login_ip_usual\n",
    "    features['is_login_city_usual'] = is_login_city_usual\n",
    "    features['is_login_type_usual'] = is_login_type_usual\n",
    "    features['is_login_device_exists_previous'] =     is_login_device_exists_previous\n",
    "    features['is_login_from_exists_previous'] =     is_login_from_exists_previous\n",
    "    features['is_login_ip_exists_previous'] = is_login_ip_exists_previous\n",
    "    features['is_login_city_exists_previous'] =     is_login_city_exists_previous\n",
    "    features['is_login_type_exists_previous'] =     is_login_type_exists_previous\n",
    "    \n",
    "    print('Shape of Features 1:', features.shape)    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_feature(login, trade, type, train_trade):\n",
    "    # basic time feature\n",
    "    login['login_real_month'] = login['timestamp'].apply(lambda x:     get_datetime(x).month)\n",
    "    login['login_real_day'] = login['timestamp'].apply(lambda x:     get_datetime(x).day)\n",
    "    login['login_real_hour'] = login['timestamp'].apply(lambda x:     get_datetime(x).hour)\n",
    "    login['login_real_minute'] = login['timestamp'].apply(lambda x:    get_datetime(x).minute)\n",
    "    login['login_real_second'] = login['timestamp'].apply(lambda x:    get_datetime(x).second)\n",
    "    \n",
    "    trade['trade_real_month'] = trade['time'].apply(lambda x:    get_datetime(x).month)\n",
    "    trade['trade_real_day'] = trade['time'].apply(lambda x:    get_datetime(x).day)\n",
    "    trade['trade_real_hour'] = trade['time'].apply(lambda x:    get_datetime(x).hour)\n",
    "    trade['trade_real_minute'] = trade['time'].apply(lambda x:    get_datetime(x).minute)\n",
    "    trade['trade_real_second'] = trade['time'].apply(lambda x:    get_datetime(x).second)\n",
    "    \n",
    "    # convert time to timestamp\n",
    "    login['time'] = login['timestamp']\n",
    "    trade['time'] = trade['time'].apply(lambda x: float(x))\n",
    "    del login['timestamp']\n",
    "    del trade['rowkey']\n",
    "    \n",
    "    login['login_result'] = login['result'].apply(lambda x: 1 if x > 0 else 0) # 1 is login sucess while others are fail\n",
    "    login['is_scan'] = login['is_scan'].apply(lambda x: 1 if x else 0)\n",
    "    login['is_sec'] = login['is_sec'].apply(lambda x: 1 if x else 0)\n",
    "    \n",
    "    # 处理test时，将训练集中的历史交易数据单独放进来提取特征\n",
    "    if type == 'test':\n",
    "        train_trade['trade_real_month'] = train_trade['time'].apply(lambda x: get_datetime(x).month)\n",
    "        train_trade['trade_real_day'] = train_trade['time'].apply(lambda x: get_datetime(x).day)\n",
    "        train_trade['trade_real_hour'] = train_trade['time'].apply(lambda x: get_datetime(x).hour)\n",
    "        train_trade['trade_real_minute'] = train_trade['time'].apply(lambda x: get_datetime(x).minute)\n",
    "        train_trade['trade_real_second'] = train_trade['time'].apply(lambda x: get_datetime(x).second)\n",
    "        train_trade['time'] = train_trade['time'].apply(lambda x: float(x))\n",
    "        del train_trade['rowkey']\n",
    "        previous_train_trade = train_trade\n",
    "    else:\n",
    "        previous_train_trade = trade\n",
    "        \n",
    "    # start build feature\n",
    "    feature_0 = build_feature_0(login, trade)\n",
    "    feature_1 = build_feature_1(login, trade)\n",
    "    \n",
    "    all_features = pd.contact([feature_0, feature_1], axis=1)\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Features 0: (132719, 6)\n",
      "Processing till line:  0\n",
      "Processing till line:  100\n",
      "Processing till line:  200\n",
      "Processing till line:  300\n",
      "Processing till line:  400\n",
      "Processing till line:  500\n",
      "Processing till line:  600\n",
      "Processing till line:  700\n",
      "Processing till line:  800\n",
      "Processing till line:  900\n",
      "Processing till line:  1000\n",
      "Processing till line:  1100\n",
      "Processing till line:  1200\n",
      "Processing till line:  1300\n",
      "Processing till line:  1400\n",
      "Processing till line:  1500\n",
      "Processing till line:  1600\n",
      "Processing till line:  1700\n",
      "Processing till line:  1800\n",
      "Processing till line:  1900\n",
      "Processing till line:  2000\n",
      "Processing till line:  2100\n",
      "Processing till line:  2200\n",
      "Processing till line:  2300\n",
      "Processing till line:  2400\n",
      "Processing till line:  2500\n",
      "Processing till line:  2600\n",
      "Processing till line:  2700\n",
      "Processing till line:  2800\n",
      "Processing till line:  2900\n",
      "Processing till line:  3000\n",
      "Processing till line:  3100\n",
      "Processing till line:  3200\n",
      "Processing till line:  3300\n",
      "Processing till line:  3400\n",
      "Processing till line:  3500\n",
      "Processing till line:  3600\n",
      "Processing till line:  3700\n",
      "Processing till line:  3800\n",
      "Processing till line:  3900\n",
      "Processing till line:  4000\n",
      "Processing till line:  4100\n",
      "Processing till line:  4200\n",
      "Processing till line:  4300\n",
      "Processing till line:  4400\n",
      "Processing till line:  4500\n",
      "Processing till line:  4600\n",
      "Processing till line:  4700\n",
      "Processing till line:  4800\n",
      "Processing till line:  4900\n",
      "Processing till line:  5000\n",
      "Processing till line:  5100\n",
      "Processing till line:  5200\n",
      "Processing till line:  5300\n",
      "Processing till line:  5400\n",
      "Processing till line:  5500\n",
      "Processing till line:  5600\n",
      "Processing till line:  5700\n",
      "Processing till line:  5800\n",
      "Processing till line:  5900\n",
      "Processing till line:  6000\n",
      "Processing till line:  6100\n",
      "Processing till line:  6200\n",
      "Processing till line:  6300\n",
      "Processing till line:  6400\n",
      "Processing till line:  6500\n",
      "Processing till line:  6600\n",
      "Processing till line:  6700\n",
      "Processing till line:  6800\n",
      "Processing till line:  6900\n",
      "Processing till line:  7000\n",
      "Processing till line:  7100\n",
      "Processing till line:  7200\n",
      "Processing till line:  7300\n",
      "Processing till line:  7400\n",
      "Processing till line:  7500\n",
      "Processing till line:  7600\n",
      "Processing till line:  7700\n",
      "Processing till line:  7800\n",
      "Processing till line:  7900\n",
      "Processing till line:  8000\n",
      "Processing till line:  8100\n",
      "Processing till line:  8200\n",
      "Processing till line:  8300\n",
      "Processing till line:  8400\n",
      "Processing till line:  8500\n",
      "Processing till line:  8600\n",
      "Processing till line:  8700\n",
      "Processing till line:  8800\n",
      "Processing till line:  8900\n",
      "Processing till line:  9000\n",
      "Processing till line:  9100\n",
      "Processing till line:  9200\n",
      "Processing till line:  9300\n",
      "Processing till line:  9400\n",
      "Processing till line:  9500\n",
      "Processing till line:  9600\n",
      "Processing till line:  9700\n",
      "Processing till line:  9800\n",
      "Processing till line:  9900\n",
      "Processing till line:  10000\n",
      "Processing till line:  10100\n",
      "Processing till line:  10200\n",
      "Processing till line:  10300\n",
      "Processing till line:  10400\n",
      "Processing till line:  10500\n",
      "Processing till line:  10600\n",
      "Processing till line:  10700\n",
      "Processing till line:  10800\n",
      "Processing till line:  10900\n",
      "Processing till line:  11000\n",
      "Processing till line:  11100\n",
      "Processing till line:  11200\n",
      "Processing till line:  11300\n",
      "Processing till line:  11400\n",
      "Processing till line:  11500\n",
      "Processing till line:  11600\n",
      "Processing till line:  11700\n",
      "Processing till line:  11800\n",
      "Processing till line:  11900\n",
      "Processing till line:  12000\n",
      "Processing till line:  12100\n",
      "Processing till line:  12200\n",
      "Processing till line:  12300\n",
      "Processing till line:  12400\n",
      "Processing till line:  12500\n",
      "Processing till line:  12600\n",
      "Processing till line:  12700\n",
      "Processing till line:  12800\n",
      "Processing till line:  12900\n",
      "Processing till line:  13000\n",
      "Processing till line:  13100\n",
      "Processing till line:  13200\n",
      "Processing till line:  13300\n",
      "Processing till line:  13400\n",
      "Processing till line:  13500\n",
      "Processing till line:  13600\n",
      "Processing till line:  13700\n",
      "Processing till line:  13800\n",
      "Processing till line:  13900\n",
      "Processing till line:  14000\n",
      "Processing till line:  14100\n",
      "Processing till line:  14200\n",
      "Processing till line:  14300\n",
      "Processing till line:  14400\n",
      "Processing till line:  14500\n",
      "Processing till line:  14600\n",
      "Processing till line:  14700\n",
      "Processing till line:  14800\n",
      "Processing till line:  14900\n",
      "Processing till line:  15000\n",
      "Processing till line:  15100\n",
      "Processing till line:  15200\n",
      "Processing till line:  15300\n",
      "Processing till line:  15400\n",
      "Processing till line:  15500\n",
      "Processing till line:  15600\n",
      "Processing till line:  15700\n",
      "Processing till line:  15800\n",
      "Processing till line:  15900\n",
      "Processing till line:  16000\n",
      "Processing till line:  16100\n",
      "Processing till line:  16200\n",
      "Processing till line:  16300\n",
      "Processing till line:  16400\n",
      "Processing till line:  16500\n",
      "Processing till line:  16600\n",
      "Processing till line:  16700\n",
      "Processing till line:  16800\n",
      "Processing till line:  16900\n",
      "Processing till line:  17000\n",
      "Processing till line:  17100\n",
      "Processing till line:  17200\n",
      "Processing till line:  17300\n",
      "Processing till line:  17400\n",
      "Processing till line:  17500\n",
      "Processing till line:  17600\n",
      "Processing till line:  17700\n",
      "Processing till line:  17800\n",
      "Processing till line:  17900\n",
      "Processing till line:  18000\n",
      "Processing till line:  18100\n",
      "Processing till line:  18200\n",
      "Processing till line:  18300\n",
      "Processing till line:  18400\n",
      "Processing till line:  18500\n",
      "Processing till line:  18600\n",
      "Processing till line:  18700\n",
      "Processing till line:  18800\n",
      "Processing till line:  18900\n",
      "Processing till line:  19000\n",
      "Processing till line:  19100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f341a343207e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train_login\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_trade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_trade\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_test_login\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_test_trade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_trade\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf_train_trade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./train_x.hdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./test_x.hdf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-cb7c3b124432>\u001b[0m in \u001b[0;36mbuild_feature\u001b[0;34m(login, trade, type, train_trade)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# start build feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mfeature_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrade\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mfeature_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrade\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-28ae1f7fe577>\u001b[0m in \u001b[0;36mbuild_feature_1\u001b[0;34m(login, trade)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# 查找交易信息中同id的记录\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mrelated_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrade\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mcount_trade_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelated_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrelated_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other, axis)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3065\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_Unspecified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moldcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseterrcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mseterr\u001b[0;34m(all, divide, over, under, invalid)\u001b[0m\n\u001b[1;32m   2761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2762\u001b[0m     \u001b[0mpyvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaskvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2763\u001b[0;31m     \u001b[0mumath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseterrobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2764\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x = build_feature(df_train_login.copy(), df_train_trade.copy(), type='train', train_trade=None)\n",
    "test_x = build_feature(df_test_login.copy(), df_test_trade.copy(), type='test', train_trade=df_train_trade.copy())\n",
    "train_x.to_hdf('./train_x.hdf', 'w')\n",
    "test_x.to_hdf('./test_x.hdf', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(df_all_login, df_target, df_other, num):\n",
    "    enc_result = OneHotEncoder()\n",
    "    enc_result.fit(np.append(df_all_login['result'].values, 0).reshape(-1, 1))\n",
    "    enc_device = OneHotEncoder()\n",
    "    enc_device.fit(np.append(df_all_login['device'].values, 0).reshape(-1, 1))\n",
    "    enc_logfrom = OneHotEncoder()\n",
    "    enc_logfrom.fit(np.append(df_all_login['log_from'].values, 0).reshape(-1, 1))\n",
    "    enc_ip = OneHotEncoder()\n",
    "    enc_ip.fit(np.append(df_all_login['ip'].values, 0).reshape(-1, 1))\n",
    "    enc_city = OneHotEncoder()\n",
    "    enc_city.fit(np.append(df_all_login['city'].values, 0).reshape(-1, 1))\n",
    "    enc_type = OneHotEncoder()\n",
    "    enc_type.fit(np.append(df_all_login['type'].values, 0).reshape(-1, 1))\n",
    "    \n",
    "    feature_1 = pd.DataFrame(enc_result.transform(df_target['last_login_result'].values.reshape(-1, 1)).toarray())\n",
    "    feature_1.columns = [\"last_login_result_\"+str(num) for num in range(1, 12)]\n",
    "    del df_target['last_login_result']\n",
    "    \n",
    "    df_target['timezone'] = df_target['last_login_real_time'].apply(process_time)\n",
    "    enc_time = OneHotEncoder()\n",
    "    enc_time.fit(df_target['timezone'].values.reshape(-1, 1))\n",
    "    feature_1_1 = pd.DataFrame(enc_time.transform(df_target['timezone'].values.reshape(-1, 1)).toarray())\n",
    "    feature_1_1.columns = ['Last_Login_TimeZone_'+str(num) for num in range(1, 6)]\n",
    "    del df_target['last_login_real_time']\n",
    "\n",
    "    df_target['timezone'] = df_target['last_trade_real_time'].apply(process_time)\n",
    "    enc_time = OneHotEncoder()\n",
    "    enc_time.fit(df_target['timezone'].values.reshape(-1, 1))\n",
    "    feature_1_2 = pd.DataFrame(enc_time.transform(df_target['timezone'].values.reshape(-1, 1)).toarray())\n",
    "    feature_1_2.columns = ['Last_Trade_TimeZone_'+str(num) for num in range(1, 6)]\n",
    "    del df_target['last_trade_real_time']\n",
    "\n",
    "    del df_target['timezone']\n",
    "\n",
    "    feature_1_3 = pd.DataFrame(enc_logfrom.transform(df_target['last_login_from'].values.reshape(-1, 1)).toarray())\n",
    "    feature_1_3.columns = [\"last_login_from_\"+str(num) for num in range(1, 13)]\n",
    "    del df_target['last_login_from']\n",
    "\n",
    "    feature_1_4 = pd.DataFrame(enc_city.transform(df_target['last_login_city'].values.reshape(-1, 1)).toarray())\n",
    "    feature_1_4.columns = [\"last_login_city_\" + str(num) for num in range(1, 486)]\n",
    "    del df_target['last_login_city']\n",
    "\n",
    "    feature_1_5 = pd.DataFrame(enc_type.transform(df_target['last_login_type'].values.reshape(-1, 1)).toarray())\n",
    "    feature_1_5.columns = [\"last_login_type_\" + str(num) for num in range(1, 5)]\n",
    "    del df_target['last_login_type']\n",
    "    \n",
    "    all_features = pd.concat([df_target, feature_1], axis=1)\n",
    "    all_features = pd.concat([all_features, feature_1_1], axis=1)\n",
    "    all_features = pd.concat([all_features, feature_1_2], axis=1)\n",
    "    all_features = pd.concat([all_features, feature_1_3], axis=1)\n",
    "    all_features = pd.concat([all_features, feature_1_4], axis=1)\n",
    "    all_features = pd.concat([all_features, feature_1_5], axis=1)\n",
    "    \n",
    "    print(all_features.info())\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Loading Data ... \")\n",
    "\n",
    "# 导入数据\n",
    "train_x, train_y, test_x = load_data()\n",
    "\n",
    "# 用sklearn.cross_validation进行训练数据集划分，这里训练集和交叉验证集比例为7：3，可以自己根据需要设置\n",
    "X, val_X, y, val_y = train_test_split(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    test_size=0.05,\n",
    "    random_state=1,\n",
    "    stratify=train_y ## 这里保证分割后y的比例分布与原数据一致\n",
    ")\n",
    "\n",
    "X_train = X\n",
    "y_train = y\n",
    "X_test = val_X\n",
    "y_test = val_y\n",
    "\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "# specify your configurations as a dict\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss', 'auc'},\n",
    "    'num_leaves': 5,\n",
    "    'max_depth': 6,\n",
    "    'min_data_in_leaf': 450,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 5,\n",
    "    'lambda_l1': 1,  \n",
    "    'lambda_l2': 0.001,  # 越小l2正则程度越高\n",
    "    'min_gain_to_split': 0.2,\n",
    "    'verbose': 5,\n",
    "    'is_unbalance': True\n",
    "}\n",
    "\n",
    "# train\n",
    "print('Start training...')\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=10000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=500)\n",
    "\n",
    "print('Start predicting...')\n",
    "\n",
    "preds = gbm.predict(test_x, num_iteration=gbm.best_iteration)  # 输出的是概率结果\n",
    "\n",
    "# 导出结果\n",
    "threshold = 0.5\n",
    "for pred in preds:\n",
    "    result = 1 if pred > threshold else 0\n",
    "\n",
    "# 导出特征重要性\n",
    "importance = gbm.feature_importance()\n",
    "names = gbm.feature_name()\n",
    "with open('./feature_importance.txt', 'w+') as file:\n",
    "    for index, im in enumerate(importance):\n",
    "        string = names[index] + ', ' + str(im) + '\\n'\n",
    "        file.write(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
